import os
import json
import glob
import re
import time
import frontmatter
from google import genai

# ========================
# é…ç½®
# ========================
api_key = os.environ.get("GEMINI_API_KEY")
if not api_key:
    raise ValueError("è«‹è¨­å®šç’°å¢ƒè®Šæ•¸ GEMINI_API_KEY")

client = genai.Client(api_key=api_key)
model_name = "gemini-2.5-flash-lite"

CONTENT_DIR = "content/blog/**/*.md"
CACHE_FILE = "data/keywords_cache.json"   # å­˜å·²è™•ç†éçš„ slug
SLEEP_BETWEEN = 10    # æ¯æ¬¡ API å‘¼å«é–“éš”ç§’æ•¸ï¼ˆå…è²»ç‰ˆå»ºè­° 5ï¼‰
MAX_RETRY = 3        # å¤±æ•—æœ€å¤šé‡è©¦å¹¾æ¬¡

# ========================
# è®€å– / å„²å­˜ Cache
# ========================
os.makedirs("data", exist_ok=True)

if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        cache = json.load(f)
    print(f"ğŸ“¦ è¼‰å…¥ cacheï¼Œå·²æœ‰ {len(cache)} ç­†è¨˜éŒ„")
else:
    cache = {}
    print("ğŸ“¦ Cache ä¸å­˜åœ¨ï¼Œå¾é ­é–‹å§‹")

def save_cache():
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

# ========================
# æ¸…ç†æ¨™ç±¤
# ========================
def clean_tags(tags_list):
    cleaned = []
    for tag in tags_list:
        t = re.sub(r'["\'\s]', '', tag.strip()).strip('ï¼Œã€‚ã€')
        if t and len(t) <= 15:
            cleaned.append(t)
    return list(set(cleaned))

# ========================
# å‘¼å« Geminiï¼ˆå«è‡ªå‹• retryï¼‰
# ========================
def get_ai_keywords(content, title):
    prompt = f"""
ä»»å‹™ï¼šä½œç‚º SEO + å…§å®¹å°ˆå®¶ï¼Œç‚ºé€™ç¯‡ç¹é«”ä¸­æ–‡éƒ¨è½æ ¼æ–‡ç« æå– 5 å€‹æœ€æ ¸å¿ƒçš„èªç¾©é—œéµå­—ã€‚
åš´æ ¼è¦å‰‡ï¼š
1. å…¨éƒ¨ç”¨ç¹é«”ä¸­æ–‡
2. åªç”¨åè©æˆ–åè©çŸ­èªï¼ˆä¾‹å¦‚ã€Œå‘é‡è³‡æ–™åº«ã€ã€ŒRAG æ‡‰ç”¨ã€ã€ŒPython è‡ªå‹•åŒ–ã€ï¼‰
3. ä¸è¦è§£é‡‹ã€ä¸è¦å‰è¨€ã€ä¸è¦ç·¨è™Ÿã€ä¸è¦å¤šé¤˜æ–‡å­—
4. è¼¸å‡ºæ ¼å¼åš´æ ¼ç‚ºï¼šé—œéµå­—1, é—œéµå­—2, é—œéµå­—3, é—œéµå­—4, é—œéµå­—5

æ–‡ç« æ¨™é¡Œï¼š{title}
æ–‡ç« å…§å®¹ï¼ˆå‰1500å­—ï¼‰ï¼š{content[:1500]}
"""

    for attempt in range(1, MAX_RETRY + 1):
        try:
            response = client.models.generate_content(
                model=model_name,
                contents=[prompt]
            )
            if response.candidates and response.candidates[0].content.parts:
                raw_text = response.candidates[0].content.parts[0].text.strip()
            else:
                raw_text = ""

            tags = re.split(r'[,ï¼Œã€\n]', raw_text)
            cleaned = clean_tags(tags)
            while len(cleaned) < 5:
                cleaned.append("å…¶ä»–")
            return cleaned[:5]

        except Exception as e:
            err = str(e)
            if "429" in err or "RESOURCE_EXHAUSTED" in err:
                wait = 60 * attempt  # ç¬¬ä¸€æ¬¡ç­‰ 60 ç§’ï¼Œç¬¬äºŒæ¬¡ 120 ç§’
                print(f"  âš ï¸ Rate limitï¼ç­‰å¾… {wait} ç§’å¾Œé‡è©¦ï¼ˆç¬¬ {attempt}/{MAX_RETRY} æ¬¡ï¼‰...")
                time.sleep(wait)
            else:
                print(f"  âŒ API éŒ¯èª¤ï¼ˆç¬¬ {attempt} æ¬¡ï¼‰ï¼š{e}")
                time.sleep(5)

    print("  âŒ è¶…éé‡è©¦æ¬¡æ•¸ï¼Œè·³éæ­¤ç¯‡")
    return []

# ========================
# ä¸»ç¨‹å¼
# ========================
files = glob.glob(CONTENT_DIR, recursive=True)
print(f"\nğŸ“‚ æ‰¾åˆ° {len(files)} ç¯‡æ–‡ç« \n")

updated_count = 0
skip_count = 0

for filepath in files:
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            post = frontmatter.load(f)

        # è·³éè‰ç¨¿
        if post.get('draft') is True:
            continue

        title = post.get('title', '')
        if not title or title.strip() == 'blog':   # â† åŠ é€™ä¸‰è¡Œ
            print(f"  â­ï¸ è·³éï¼ˆç„¡æ¨™é¡Œï¼‰ï¼š{filepath}")
            continue

        # è·³éå·²æœ‰ ai_keywords çš„æ–‡ç« ï¼ˆfrontmatter è£¡æœ‰ï¼‰
        if post.get('ai_keywords'):
            skip_count += 1
            continue

        # è·³é cache è£¡å·²è™•ç†éçš„ï¼ˆä½†é‚„æ²’å¯«é€² md çš„å‚™æ´ï¼‰
        slug = os.path.splitext(os.path.basename(filepath))[0]
        if slug == "index":
            slug = os.path.basename(os.path.dirname(filepath))

        if slug in cache:
            # Cache æœ‰ä½† md æ²’å¯«åˆ° â†’ è£œå¯«é€²å»
            print(f"ğŸ”„ å¾ cache è£œå¯«ï¼š{title}")
            post['ai_keywords'] = cache[slug]
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(frontmatter.dumps(post))
            updated_count += 1
            continue

        # å‘¼å« Gemini
        print(f"ğŸ¤– è™•ç†ä¸­ï¼š{title}")
        keywords = get_ai_keywords(post.content, title)

        if keywords:
            # å¯«é€² md
            post['ai_keywords'] = keywords
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(frontmatter.dumps(post))

            # åŒæ­¥å­˜é€² cache
            cache[slug] = keywords
            save_cache()

            print(f"  âœ… {keywords}")
            updated_count += 1
        else:
            print(f"  âš ï¸ ç„¡é—œéµå­—ï¼Œè·³é")

        time.sleep(SLEEP_BETWEEN)

    except Exception as e:
        print(f"âŒ è™•ç†å¤±æ•— {filepath}ï¼š{e}")

print(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… å®Œæˆï¼
   æ–°å¢è™•ç†ï¼š{updated_count} ç¯‡
   å·²æœ‰è·³éï¼š{skip_count} ç¯‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸‹ä¸€æ­¥ï¼š
  python scripts/generate_related.py
  hugo --minify
""")
